---
title: "Homework 5"
author: "Neal Marquez"
date: "Due: Tues, 11/21/2017"
output: html_document
---

# Exercise 1 

In this exercise, we will use `unemployment_pp.csv`, the depression and unemployment dataset discussed in lectures and Lab 7. We will model the trajectory of depression (measured on the CES-D scale) as a linear function of time-varying unemployment status and the interaction between time and unemployment status. In our multilevel model, we will have random intercepts and a random slope for the interaction between time and unemployment, but only a fixed effect for the unemployment status. (This is the "simpler model" discussed at the end of Lab 7 - see the updated Lab 7 sheet on Canvas.)

### Problem 1.1 

```{R warning=FALSE,message=FALSE,error=FALSE}
rm(list=ls())
library(nlme)
library(tidyverse)
library(parallel)
library(pander)
library(mvtnorm)
library(ggplot2)
set.seed(123)
```

*Write out this model in both two-level and composite forms. Fit this model to the unemployment data.*

\begin{aligned}
\text{Two Level Form} \\
\log (Y_{ij}) = & ~ \pi_{0i} + \gamma_{10} \cdot (\text{unemployed}_{ij}) + 
               \pi_{2i} \cdot (\text{unemployed}_{ij} \cdot 
               \text{time}_{ij}) + \varepsilon_{ij}\\ 
\pi_{0i} = & ~ \gamma_{00} + \zeta_{0i} \\
\pi_{2i} = & ~ \gamma_{20} + \zeta_{2i} \\

\text{Composite Form} \\
\log(Y_{ij}) = & ~\gamma_{00} + \gamma_{10} \cdot (\text{unemployed}_{ij}) + 
               \gamma_{20} \cdot (\text{unemployed}_{ij} \cdot 
               \text{time}_{ij}) + \\
               & ~\zeta_{0i} + \zeta_{2i} \cdot 
               (\text{unemployed}_{ij} \cdot \text{time}_{ij}) +
               \varepsilon_{ij} \\

\text{Variance Params} \\
\varepsilon_{ij} \sim_{iid} & ~ \mathcal{N}(0, \sigma_{\varepsilon}) \\
\end{aligned}

$$
\
\begin{bmatrix}
   \zeta_{0i} \\
   \zeta_{2i} \\
\end{bmatrix} 
\
\sim \mathcal{N} \Bigg(
\
\begin{bmatrix}
   0 \\
   0 \\
\end{bmatrix} 
\
,
\begin{bmatrix}
   \sigma_0^2 & \sigma_{02} \\
   \sigma_{02} & \sigma_2^2 \\
\end{bmatrix} 
\
\Bigg)
$$

```{R SimpleModel}
unemployment <- read.table("./unemployment_pp.csv", header=TRUE, sep=",")

simpler_model <- lme(fixed= cesd ~ 1 + unemp + unemp:months,
                     data=unemployment, random= ~ 1 + unemp:months | id)
summary(simpler_model)
```

*What proportion of the total variability in depression outcomes can be explained by the variation in each of the three variance components? Report this out in a (nicely-formatted) table.*

#### Proportion of Variance Explained

```{R SimpleVarianceExplained}
var_explained <- VarCorr(simpler_model)[,1] %>% 
    as.numeric %>% `/`(var(unemployment$cesd)) %>% round(4)
names(var_explained) <- c("Intercept", "Unemployment x Months", "Residual")
pander(var_explained)
```

### Problem 1.2 

*Write a function to simulate outcome data from this model. The function takes as inputs the original data frame and model object and needs to*

* *obtain the structural part of the model*
* *obtain the random part of the model, using the distribution estimated in Problem 1.1*
* *obtain the residual error part of the model, using the $\sigma_{\epsilon}^2$ estimated in Problem 1.1*
* *combine the above components to calculate the simulated outcome value*
* *return the original data frame with **only one** extra column for the simulated outcomes.*

```{R simulate_obs}

simulate_obs <- function(DF, model=simpler_model){
    ## obtain the structural part of the model
    sim_data <- DF %>%
        mutate(fixed_pred=predict(model, newdata=., level=0))
    
    ## obtain residual error
    sigma_epsilon <- model$sigma
    sim_data <- sim_data %>% 
        mutate(resid_err=rnorm(nrow(.), sd=sigma_epsilon))
    
    ## obtain the random part of the model and simulate
    re_vcov <- model %>% getVarCov %>% unclass
    RE_sim <- rmvnorm(n=length(unique(sim_data$id)), sigma=re_vcov) %>%
        as.data.frame %>% 
        setNames(c("zeta_0i", "zeta_2i")) %>%
        mutate(id=unique(sim_data$id))
    
    ## combine all the parts
    sim_data_pred <- sim_data %>%
        left_join(RE_sim, by="id") %>%
        mutate(ran_pred=zeta_0i + zeta_2i*unemp*months) %>%
        mutate(pred=ran_pred + fixed_pred + resid_err)
    
    ## return the preds with just the one extra column
    sim_data_pred %>% select(c(names(DF), "pred"))
}
```

### Problem 1.3

*We want to understand the effect that re-employment had on depression, using both simulated and observed data. We will consider two archetypes in the next few problems:* 

* *archetype 1: subjects who remained unemployed (pattern "1, 1, 1" in variable `unemp`)* 
* *archetype 2: subjects who started unemployed but became and stayed employed in the subsequent time points (pattern "1, 0, 0" in variable `unemp`). *

*Generate 1000 individuals from archetypes 1 and 2 (use the same seed and code format below for the time points) and simulate depression data for them with your function from Problem 1.2. Calculate the difference in depression scores at the 3rd time points between those in archetypes 1 and 2. Do this vector-wise to generate 1000 such differences.*

*Hint: one easy way to pull out the nth value of, say, the `unemp` variable for each subject in the unemployment data is `unemployment %>% group_by(id) %>% summarize(nth_msmt = nth(unemp, n))`.*

```{R archetypes_simulate}
set.seed("823547")
sample_subjs_archetype1 <- rbind(expand.grid(id=1:1000,
                            unemp=1,
                            months=quantile(unemployment$months, 0.2)),
                      expand.grid(id=1:1000,
                            unemp=1,
                            months=quantile(unemployment$months, 0.5)),
                      expand.grid(id=1:1000,
                            unemp=1,
                            months=quantile(unemployment$months, 0.8))) %>%
    mutate(archetype=1)

# Now generate 1000 individuals from archetype 2...
sample_subjs_archetype2 <- rbind(expand.grid(id=1:1000,
                            unemp=1,
                            months=quantile(unemployment$months, 0.2)),
                      expand.grid(id=1:1000,
                            unemp=0,
                            months=quantile(unemployment$months, 0.5)),
                      expand.grid(id=1:1000,
                            unemp=0,
                            months=quantile(unemployment$months, 0.8))) %>%
    mutate(archetype=2)

archs_simulated <- rbind(
    sample_subjs_archetype1 %>% simulate_obs %>% 
        group_by(id) %>% summarize_all(function(y) nth(y, n=3)),
    sample_subjs_archetype2 %>% simulate_obs %>% 
        group_by(id) %>% summarize_all(function(y) nth(y, n=3)))

arch_diff <- archs_simulated %>% 
    select(id, archetype, pred) %>%
    spread(archetype, pred) %>% 
    mutate(diff=`2` - `1`)
```

### Problem 1.4

*Extract out from the real data subjects in archetypes 1 and 2. How many of each archetype do we have? What is the difference in the average depression score at the 3rd time point between the two archetypes? Interpret this value.*

```{R archetpes_observed}
arch1_ids <- intersect(
    unemployment %>% group_by(id) %>% 
    summarize(second=nth(unemp, 2)) %>%
    filter(second==1) %>% select(id),
    unemployment %>% group_by(id) %>% 
    summarize(third=nth(unemp, 3)) %>%
    filter(third==1) %>% select(id)) %>% unlist

arch2_ids <- intersect(
    unemployment %>% group_by(id) %>% 
    summarize(second=nth(unemp, 2)) %>%
    filter(second==0) %>% select(id),
    unemployment %>% group_by(id) %>% 
    summarize(third=nth(unemp, 3)) %>%
    filter(third==0) %>% select(id)) %>% unlist

# sanity check these results
filter(unemployment, id %in% arch1_ids) %>% select(unemp) %>%
    unlist %>% set_names(NULL) %>% all.equal(.,rep(1, length(.)))

filter(unemployment, id %in% arch2_ids) %>% arrange(id, months) %>%
    select(unemp) %>% unlist %>% set_names(NULL) %>% 
    all.equal(., rep(c(1,0,0), length(.)/3))

third_time_cesd_arch1 <- filter(unemployment, id %in% arch1_ids) %>%
    group_by(id) %>% summarize(cesd3=nth(cesd,3)) %>%
    select(cesd3) %>% unlist %>% mean

third_time_cesd_arch2 <- filter(unemployment, id %in% arch2_ids) %>%
    group_by(id) %>% summarize(cesd3=nth(cesd,3)) %>%
    select(cesd3) %>% unlist %>% mean

obs_3rd_diff <- third_time_cesd_arch2 - third_time_cesd_arch1
sim_3rd_diff <- mean(arch_diff$diff)
```

In our observed dataset we have `r length(arch1_ids)` individuals in the first archetype and `r length(arch2_ids)` individuals of the second archetype. At the third time point observed for each individual the observed difference between the mean of archetype 2 individuals and teh mean of archetype 1 individuals was `r round(obs_3rd_diff, 2)`. It is important to note that individuals in the obsereved set were all observed at different times, time is a continuous and unscheduled variable, while in the simulated data set all individuals were simulated at the same time point. Nevertheless, the value states that at the third time point, on average, we see lower values of HamD depression score for archetype 2 than archetype 1. To ensure that this is a fair comparison we should check that there is not a significant difference in the third time observed between the individuals in archetype 2 and archetype 1.

### Problem 1.5 

*Plot a histogram of the 1000 simulated differences calculated in Problem 1.3. Comment on the shape of this distribution. *

*Calculate the mean difference and generate an approximate 95% confidence interval for the true difference in depression corresponding to maintaining employment, among the simulated subjects (use the `quantile` function). Interpret the mean and confidence interval. Based on your simulations, does maintaining employment over time result in a change in depression at the 3rd time point? Why or why not? How does this compare with your result in Problem 1.4?*

```{R analyze_diff_results}
ggplot(data=arch_diff, aes(x=diff)) + geom_histogram(bins=30) + 
     geom_vline(aes(xintercept=mean(diff, na.rm=T)),
               color="red", linetype="dashed", size=1) +
    labs(title="Simulated Differnces Between Archetypes at Time 3",
         x="Predict CESD for Archetype 2 - Archetype 1")

diffCI <- quantile(arch_diff$diff, c(.025, .975))
pander(diffCI)
```

Above is a histogram of the simulated values for the differnce in HamD scores for the third time observed between archetype 2 and archetype 1 individuals. The shape of the distribution is fairly normal which makes sense since each component of the simulated values is either a fixed scalar value or normal distribution and the product/sum of a scalar and a normal distribution and the sum of normal distributions is also a normal distribution. 

The estimated value of the differnce at the third time point is `r round(sim_3rd_diff, 2)` with confidence interval (`r paste(round(diffCI, 2), collapse=", ")`). The mean value is negative which means that at the third time point simulated individuals of archetype 2 had a lower value of HamD than those in archetype 1, in line with our observed data. The confidence intervals for this value is relatively large and crosses the zero value which indicates that although the simulated differnce is on average negative, that there is a large amount of variablity amongst individuals within each group.

# Exercise 2 

Here, we return to our dataset on depression severity of patients and consider growth models with different orders of the time variable.

### Problem 2.1

*Using dataset `depression_hw5.csv`, explore a subset of 8 individual trajectories of depression severity scores, with subject IDs 120, 319, 339, 346, 351, 353, 604, and 606. Is there some support for curve-linear functional forms of the individual trajectories?*

```{R explore_depression}
depression <- read.table("./depression_hw5.csv", sep=",", header=TRUE) %>%
    mutate(Period0=Period-1, P2=Period0**2, P3=Period0**3)

inspect_subjects <- c(120, 319, 339, 346, 351, 353, 604, 606)

filter(depression, Subject %in% inspect_subjects & !is.na(HamD)) %>%
    ggplot(aes(Period, HamD, color=Sex)) + 
    geom_point() + facet_wrap(~Subject) + 
    labs(title="HamD Over Periods By Subject ID")

```

A look at the data over time for our subset of samples shows that we have at most two four data points for each period making it heard to argue for more than three parameters to fit each trajectory. There does seem to be some indication of a non-linear fit, however, so at the very least testing a model with three parameters should be done. 

### Problem 2.2 

*Starting with the unconditional means model, develop a taxonomy of models by adding the quadratic and cubic time components and associated random effects (e.g., see Table 6.5 in Singer and Willett). Build the following models*

* *Unconditional means*
* *Linear growth model*
* *Quadratic growth model*
* *Cubic growth model*

*You will need to adjust some of the `lmeControl` options to get some of these models to get fit to the data. Please report what you adjusted and for which model(s).*

```{R model_building}
models <-list(
    lme(fixed=HamD ~ 1, data=depression, 
        random=~1|Subject, na.action=na.omit, method="ML"),
    lme(fixed=HamD ~ 1 + Period0, data=depression, 
        random=~1 + Period0|Subject, na.action=na.omit, method="ML"),
    lme(fixed=HamD ~ 1 + Period0 + P2, data=depression, 
        random=~1 + Period0 + P2|Subject, na.action=na.omit, method="ML"),
    lme(fixed=HamD ~ 1 + Period0 + P2 + P3, data=depression, na.action=na.omit,
        random=~1 + Period0 + P2 + P3|Subject, method="ML",
        control=lmeControl(maxIter=500,msMaxIter=500,msMaxEval=300)))

sigma2_ep <- sapply(models, function(x) (x$sigma)**2) %>% round(2)
bc <- lapply(models, function(x) x$coefficients$fixed %>% round(2))
rv <- lapply(models, function(x) getVarCov(x) %>% diag %>% round(2))
```

All models fit without alteration except for the cubic model which required increases in the default values of `maxIter`, `maMaxIter`, and `msMaxEval` in order to properly converge.

### Problem 2.3

*Write a taxonomy of the models built in Problem 2.2. Create a nicely-formatted table similar to Table 6.5 in Singer and Willett to describe the parameters in the models and the estimates you obtained.*

#### Fixed Parameter Estimates

| Parameter             |  Interpretation            |Unconditional Mean| Linear           | Quadratic        |         Cubic    |
|-----------------------|----------------------------|------------------|------------------|------------------|------------------|
| $\gamma_{00}$         |Intercept (Period 1 Status) | `r bc[[1]][[1]]` | `r bc[[2]][[1]]` | `r bc[[3]][[1]]` | `r bc[[4]][[1]]` |
| $\gamma_{10}$         |Time (linear term)          |                  | `r bc[[2]][[2]]` | `r bc[[3]][[2]]` | `r bc[[4]][[2]]` |
| $\gamma_{20}$         |Time^2 (quadratic term)     |                  |                  | `r bc[[3]][[3]]` | `r bc[[4]][[3]]` |
| $\gamma_{30}$         |Time^3 (cubic term)         |                  |                  |                  | `r bc[[4]][[4]]` |
| $\sigma_{\epsilon}^2$ |Within Individual Error Var.| `r sigma2_ep[1]` | `r sigma2_ep[2]` | `r sigma2_ep[3]` | `r sigma2_ep[4]` |
| $\sigma_{ii}^2$       |Variance of $\zeta_{i0}$ terms| See Below      |                  |                  |                  |
| $\sigma_{ij}$         |CoVariance of $\zeta_{i0}$ & $\zeta_{j0}$ terms|  See Below |     |                  |                  |

#### Variance & CoVariance Estimates

##### Unconditional Mean Model

```{R echo=FALSE}
models[[1]] %>% getVarCov %>% unclass %>% pander
```

##### Linear Model

```{R echo=FALSE}
models[[2]] %>% getVarCov %>% unclass %>% pander
```

##### Quadratic Model

```{R echo=FALSE}
models[[3]] %>% getVarCov %>% unclass %>% pander
```

##### Cubic Model

```{R echo=FALSE}
models[[4]] %>% getVarCov %>% unclass %>% pander
```

### Problem 2.4 

*Compare the models in the taxonomy using the appropriate metrics and considerations of the fitting process. Report the results of the model comparison in a nicely-formatted table.* 

```{R model_evaluate}
anova(models[[1]], models[[2]], models[[3]], models[[4]]) %>% 
    select(-call) %>% remove_rownames %>%
    mutate(Model=c("Unconditional Means",
                   "Linear Model",
                   "Quadratic Model",
                   "Cubic Model")) %>% pander
```

*Describe the best fitting model. Interpret the fixed effects estimates (do not print the model summary). Is there evidence for curve-linear functional form of the mean depression score trajectory across all individuals in the study?*

While there is no model which is the best by all criteira the Quadradic model has the lowest AIC and second lowest (behind the linear model) BIC, while also being significantly different in the chi-sqaured test. Looking at the fixed effects the parameters $\gamma_{00}$ (the intercept term) and $\gamma_{10}$ (the linear chnange term) did not drastically change as shown in the table above. In addition the parameter $\gamma_{20}$ (the quadradic term) did not significantly differ from zero which indicates that the trajectory of depression scores over periods across all individuals is close to linear. I would also be wary of selecting the cubic model because of the high colinearity of the fixed parameters and the algorith tweaking that was required in order to have the model converge. 

### Problem 2.5

*Make a plot of the mean predicted trajectory (using your preferred model) and overlay 9 predicted individual trajectories for subject IDs in Problem 2.1. Comment on the functional form of the mean depression score trajectory and of the individual fitted trajectories.*

```{R trajectories_plotted}
preds <- depression %>% 
    mutate(pred_HamD=predict(models[[3]], newdata=., level=1),
           Type="Subject", LS=1)
pred_global <- 
    data.frame(Period=1:4, Period0=0:3, P2=(0:3)^2, P3=(0:3)^3) %>%
    mutate(pred_HamD=predict(models[[3]], newdata=., level=0),
           Subject=0, Type="Global", LS=1.1)


filter(preds, Subject %in% inspect_subjects) %>% bind_rows(pred_global) %>%
    ggplot(aes(x=Period, y=pred_HamD, group=Subject, linetype=Type, size=Type)) + 
    geom_line() + 
    scale_size_manual(values=c(Global=1.8, Subject=.6),
                      label=c(Global="Population \nAverage", 
                              Subject="Subject \nEstimate")) +
    scale_linetype_discrete(label=c(Global="Population \nAverage", 
                                    Subject="Subject \nEstimate")) + 
    labs(x="Period", y="HamD Estimate", title="HamD Estimates Over Time")
```

The mean (population) score trajectory appears to be linear in its form, however individuals, more often than not, appear to have a trajetory that is better described by a quadratic fit. 